{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26ed8ba-1c74-4cf5-a901-7873fdbf273b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home2/akshett.jindal/miniconda3\n",
      "subba                 *  /home2/akshett.jindal/miniconda3/envs/subba\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9be46aa1-4e8f-4914-95be-e7a4fa2c9baf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers torch pillow torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0682f3e8-08a7-4273-b31e-42d4ca86f6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VIT_PRETRAINED_MODEL = \"google/vit-base-patch16-224\"\n",
    "BASE_DIRECTORY = \"/tmp/akshett.jindal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10feac97-d5a2-473a-bb0b-a5b146ff7495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HF_CACHE_DIR = os.path.join(BASE_DIRECTORY, \".huggingface_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7b0ff1-d0bd-484d-93a2-3b302dd80ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d06ec3-9df9-4409-9d0a-6556a498a788",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41771c37-6f52-4d6d-bec6-13790e734a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf73735-1f87-4835-9205-78869d78b414",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Loading dataset and creating a DataLoader for batching and converting to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32ee0e8-acc6-4002-8738-e30731a92af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of voxels = 15724\n"
     ]
    }
   ],
   "source": [
    "import nsd_dataset.mind_eye_nsd_utils as menutils\n",
    "\n",
    "image_dataset = menutils.load_image_dataset(BASE_DIRECTORY)\n",
    "session_data, (trn_stim_ordering, trn_voxel_data, val_stim_ordering, val_voxel_data) = menutils.get_split_data(BASE_DIRECTORY, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129d001f-eb38-4f90-8ae9-fede5c5183ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dataset => (73000, 227, 227, 3)\n",
      "session_data => (30000, 15724)\n",
      "trn_stim_ordering => (27000,)\n",
      "trn_voxel_data => (27000, 15724)\n",
      "val_stim_ordering => (3000,)\n",
      "val_voxel_data => (3000, 15724)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "image_dataset => (73000, 227, 227, 3)\n",
    "session_data => (30000, 15724)\n",
    "trn_stim_ordering => (27000,)\n",
    "trn_voxel_data => (27000, 15724)\n",
    "val_stim_ordering => (3000,)\n",
    "val_voxel_data => (3000, 15724)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"image_dataset => {image_dataset.shape}\")\n",
    "print(f\"session_data => {session_data.shape}\")\n",
    "print(f\"trn_stim_ordering => {trn_stim_ordering.shape}\")\n",
    "print(f\"trn_voxel_data => {trn_voxel_data.shape}\")\n",
    "print(f\"val_stim_ordering => {val_stim_ordering.shape}\")\n",
    "print(f\"val_voxel_data => {val_voxel_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748d7230-a50c-4ada-b9f0-db7bbbae6c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class FMRIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_order, images, voxel_data):\n",
    "        self.images = images[image_order]\n",
    "        self.voxel_data = voxel_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.voxel_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return {\n",
    "            'image': torch.tensor(self.images[idx], dtype=torch.float),\n",
    "            'fmri': torch.tensor(self.voxel_data[idx], dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25105404-dcea-4d18-a18f-4faccb367387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 3000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = FMRIDataset(trn_stim_ordering, image_dataset, trn_voxel_data)\n",
    "test_dataset = FMRIDataset(val_stim_ordering, image_dataset, val_voxel_data)\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1703b54-9e0f-4374-9c50-9320389ee493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x14fa34e28070>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x14fa34e2ab00>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': BATCH_SIZE\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, **dataloader_kwargs)\n",
    "test_dataloader = DataLoader(test_dataset, **dataloader_kwargs)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f01ca-23f2-4564-9243-de14cec253ef",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Creating a class for our model and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c12daf55-0cc7-4b98-b20f-edb628cd2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViTFMRI(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        vit_pretrained: str,\n",
    "        final_out_dim: int,\n",
    "        *args, **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._vit_pretrained = vit_pretrained\n",
    "        self._out_dim = final_out_dim\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(vit_pretrained, *args, **kwargs)\n",
    "        self.linear = nn.Linear(self.vit.config.hidden_size, final_out_dim)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        vit_output = self.vit(*args, **kwargs)\n",
    "        final_output = self.linear(vit_output['pooler_output'])\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "047a369a-a8b3-4cdd-91f6-0eca7af23cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTFMRI(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=15724, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTFMRI(\n",
    "    vit_pretrained=VIT_PRETRAINED_MODEL,\n",
    "    final_out_dim=session_data.shape[1],\n",
    "    cache_dir=HF_CACHE_DIR,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67155d8e-0294-45c5-9354-0eb25a7ee7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\n",
    "    VIT_PRETRAINED_MODEL,\n",
    "    cache_dir=HF_CACHE_DIR,\n",
    ")\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d181d-0a34-4f4b-a205-cfaa0155f89d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "66ce0e12-b2fe-4b97-ab8c-5dd1053d357c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def rankdata_ordinal(a):\n",
    "\n",
    "    arr = torch.ravel(a)\n",
    "    sorter = torch.argsort(arr, stable=True)\n",
    "\n",
    "    inv = torch.empty(sorter.shape, dtype=torch.int)\n",
    "    inv[sorter] = torch.arange(*sorter.shape, dtype=torch.int)\n",
    "\n",
    "    result = inv + 1\n",
    "    return result\n",
    "\n",
    "def rdc_tensor(x, y, f=torch.sin, k=20, s=1/6., n=1):\n",
    "\n",
    "    if n > 1:\n",
    "        values = []\n",
    "        for i in range(n):\n",
    "            try:\n",
    "                values.append(rdc(x, y, f, k, s, 1))\n",
    "            except Exception as ex:\n",
    "                pass\n",
    "        return torch.median(values)\n",
    "\n",
    "    if x.ndim == 1: x = x.reshape((-1, 1))\n",
    "    if y.ndim == 1: y = y.reshape((-1, 1))\n",
    "\n",
    "    # Copula Transformation\n",
    "    cx = torch.column_stack([rankdata_ordinal(xc) for xc in x.T])/float(x.shape[0])\n",
    "    cy = torch.column_stack([rankdata_ordinal(yc) for yc in y.T])/float(y.shape[0])\n",
    "\n",
    "    # Add a vector of ones so that w.x + b is just a dot product\n",
    "    O = torch.ones(cx.shape[0])\n",
    "    X = torch.column_stack([cx, O])\n",
    "    Y = torch.column_stack([cy, O])\n",
    "\n",
    "    # Random linear projections\n",
    "    Rx = (s/X.shape[1])*torch.randn(X.shape[1], k)\n",
    "    Ry = (s/Y.shape[1])*torch.randn(Y.shape[1], k)\n",
    "    X = torch.mm(X, Rx)\n",
    "    Y = torch.mm(Y, Ry)\n",
    "\n",
    "    # Apply non-linear function to random projections\n",
    "    fX = f(X)\n",
    "    fY = f(Y)\n",
    "\n",
    "    # Compute full covariance matrix\n",
    "    C = torch.cov(torch.hstack([fX, fY]).T)\n",
    "\n",
    "    # Due to numerical issues, if k is too large,\n",
    "    # then rank(fX) < k or rank(fY) < k, so we need\n",
    "    # to find the largest k such that the eigenvalues\n",
    "    # (canonical correlations) are real-valued\n",
    "    k0 = k\n",
    "    lb = 1\n",
    "    ub = k\n",
    "    while True:\n",
    "\n",
    "        # Compute canonical correlations\n",
    "        Cxx = C[:k, :k]\n",
    "        Cyy = C[k0:k0+k, k0:k0+k]\n",
    "        Cxy = C[:k, k0:k0+k]\n",
    "        Cyx = C[k0:k0+k, :k]\n",
    "\n",
    "        eigs = torch.linalg.eigvals(torch.mm(torch.mm(torch.linalg.pinv(Cxx), Cxy),\n",
    "                                              torch.mm(torch.linalg.pinv(Cyy), Cyx)))\n",
    "\n",
    "        # Binary search if k is too large\n",
    "        if not (torch.all(torch.isreal(eigs)) and\n",
    "                0 <= torch.min(torch.abs(eigs)) and\n",
    "                torch.max(torch.abs(eigs)) <= 1):\n",
    "            ub -= 1\n",
    "            k = (ub + lb) // 2\n",
    "            continue\n",
    "        if lb == ub: break\n",
    "        lb = k\n",
    "        if ub == lb + 1:\n",
    "            k = ub\n",
    "        else:\n",
    "            k = (ub + lb) // 2\n",
    "\n",
    "    return torch.sqrt(torch.max(torch.abs(eigs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "afb141d6-0679-4fd7-a724-a710b5bd4c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "a2 = torch.tensor([5, 4, 3, 2, 1])\n",
    "\n",
    "print(rdc_tensor(a1, a2))\n",
    "\n",
    "del a1\n",
    "del a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f351b245-5b5a-4adb-b7e4-6d52d1229345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def rdc(x, y, f=np.sin, k=20, s=1/6., n=1):\n",
    "    \"\"\"\n",
    "    Computes the Randomized Dependence Coefficient\n",
    "    x,y: numpy arrays 1-D or 2-D\n",
    "         If 1-D, size (samples,)\n",
    "         If 2-D, size (samples, variables)\n",
    "    f:   function to use for random projection\n",
    "    k:   number of random projections to use\n",
    "    s:   scale parameter\n",
    "    n:   number of times to compute the RDC and\n",
    "         return the median (for stability)\n",
    "\n",
    "    According to the paper, the coefficient should be relatively insensitive to\n",
    "    the settings of the f, k, and s parameters.\n",
    "    \"\"\"\n",
    "    if n > 1:\n",
    "        values = []\n",
    "        for i in range(n):\n",
    "            try:\n",
    "                values.append(rdc(x, y, f, k, s, 1))\n",
    "            except np.linalg.linalg.LinAlgError: pass\n",
    "        return np.median(values)\n",
    "\n",
    "    if len(x.shape) == 1: x = x.reshape((-1, 1))\n",
    "    if len(y.shape) == 1: y = y.reshape((-1, 1))\n",
    "\n",
    "    # Copula Transformation\n",
    "    cx = np.column_stack([rankdata(xc, method='ordinal') for xc in x.T])/float(x.size)\n",
    "    cy = np.column_stack([rankdata(yc, method='ordinal') for yc in y.T])/float(y.size)\n",
    "\n",
    "    # Add a vector of ones so that w.x + b is just a dot product\n",
    "    O = np.ones(cx.shape[0])\n",
    "    X = np.column_stack([cx, O])\n",
    "    Y = np.column_stack([cy, O])\n",
    "\n",
    "    # Random linear projections\n",
    "    Rx = (s/X.shape[1])*np.random.randn(X.shape[1], k)\n",
    "    Ry = (s/Y.shape[1])*np.random.randn(Y.shape[1], k)\n",
    "    X = np.dot(X, Rx)\n",
    "    Y = np.dot(Y, Ry)\n",
    "\n",
    "    # Apply non-linear function to random projections\n",
    "    fX = f(X)\n",
    "    fY = f(Y)\n",
    "\n",
    "    # Compute full covariance matrix\n",
    "    C = np.cov(np.hstack([fX, fY]).T)\n",
    "\n",
    "    # Due to numerical issues, if k is too large,\n",
    "    # then rank(fX) < k or rank(fY) < k, so we need\n",
    "    # to find the largest k such that the eigenvalues\n",
    "    # (canonical correlations) are real-valued\n",
    "    k0 = k\n",
    "    lb = 1\n",
    "    ub = k\n",
    "    while True:\n",
    "\n",
    "        # Compute canonical correlations\n",
    "        Cxx = C[:k, :k]\n",
    "        Cyy = C[k0:k0+k, k0:k0+k]\n",
    "        Cxy = C[:k, k0:k0+k]\n",
    "        Cyx = C[k0:k0+k, :k]\n",
    "\n",
    "        eigs = np.linalg.eigvals(np.dot(np.dot(np.linalg.pinv(Cxx), Cxy),\n",
    "                                        np.dot(np.linalg.pinv(Cyy), Cyx)))\n",
    "        # Binary search if k is too large\n",
    "        if not (np.all(np.isreal(eigs)) and\n",
    "                0 <= np.min(eigs) and\n",
    "                np.max(eigs) <= 1):\n",
    "            ub -= 1\n",
    "            k = (ub + lb) // 2\n",
    "            continue\n",
    "        if lb == ub: break\n",
    "        lb = k\n",
    "        if ub == lb + 1:\n",
    "            k = ub\n",
    "        else:\n",
    "            k = (ub + lb) // 2\n",
    "\n",
    "    return np.sqrt(np.max(eigs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b255f721-c180-4fee-86c3-0069dbacfb5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998408164605007\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([1, 2, 3, 4, 5])\n",
    "a2 = np.array([3124, 234, 2345, 54, 543])\n",
    "\n",
    "print(rdc(a1, a2))\n",
    "\n",
    "del a1\n",
    "del a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dedaa7e4-9720-4e13-a38d-39bb1f565197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "loss_function = rdc_tensor\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69a0f8-df84-4661-b118-9fa2d899e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81895cbe-6c0e-4d91-919c-0c45a5451de5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4db0c281b144476900d040ddc06b2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a681f85b4a714ccfa453892c86d92cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batch:   0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 227, 227, 3])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "EPOCHS = []\n",
    "TRAIN_LOSSES = []\n",
    "TEST_LOSSES = []\n",
    "\n",
    "for epoch_num in tqdm(range(1, NUM_EPOCHS+1), desc=\"Epochs\", position=0):\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    EPOCHS.append(epoch_num)\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_num, data_batch in enumerate(tqdm(train_dataloader, desc=\"Train Batch\", position=1, leave=False)):\n",
    "\n",
    "        input_images = data_batch['image']\n",
    "        fmris = data_batch['fmri']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = processor(input_images, return_tensors=\"pt\")\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "\n",
    "        outputs = model(**inputs).cpu()\n",
    "\n",
    "        loss = loss_function(outputs, fmris)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b1982-5130-4241-ae91-19132831346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627dff93-3b42-47be-9a50-4fe51843c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in inputs.keys():\n",
    "    inputs[k] = inputs[k].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05d9bc-250b-46ba-bc8b-9379c4d92aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['pixel_values'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7fb0a-7364-4029-91c7-b04f4067daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a24d6-3034-4e79-9f15-d78903b0314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d4786-6b51-46cc-8d0a-6ed62badc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pooler_output:\", outputs['pooler_output'].shape)\n",
    "print(\"last_hidden_state:\", outputs['last_hidden_state'].shape)\n",
    "\n",
    "\"\"\"\n",
    "pooler_output: torch.Size([1, 768])\n",
    "last_hidden_state: torch.Size([1, 197, 768])\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
