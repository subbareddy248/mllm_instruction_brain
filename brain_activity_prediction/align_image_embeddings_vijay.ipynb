{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55af9e0-a92b-4cd9-beea-bd904de84c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes\n",
    "# for VGG16 and SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d3a8bf-0d92-4b5e-94f5-82ddd14861f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/anaconda3/envs/pytorch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8fd20f-7fc5-4b1d-9728-c7eda647c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../resources/algonauts_2023_challenge_data'\n",
    "parent_submission_dir = '../resources/algonauts_2023_challenge_submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "798c8d11-710b-4415-ab3d-92cd717733d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca669cf0-5618-4eb5-8ee4-406c98ca6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1ee1ad-0cbc-48e9-9e91-45aeab843c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data path\n",
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "\n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)\n",
    "args.subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24633bd6-c6f0-4f26-93ed-7363c8e6aa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "# load fMRI training data\n",
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec2bf8f-c435-4573-b82f-fb12adb59b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    }
   ],
   "source": [
    "# images\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863303f9-830a-4145-bfe3-84040e058b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "# train, test, validation\n",
    "rand_seed = 5 #@param\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a030bbac-54e3-46a6-bafd-f17b9f018b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8800,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since pca fit was not working with dimension 8857\n",
    "idxs_train = idxs_train[:8800]\n",
    "idxs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e5407b-84b1-4507-ad5f-95e67f16bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), # resize the images to 224x24 pixels\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6181af8-3db4-42a7-84d3-9d4f54b88501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            img = self.transform(img).to(device)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9e76370-d4c3-4654-bf65-f6f7ec0c1c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "batch_size = 100 #@param\n",
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_train, transform),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_val, transform),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test, transform),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd743c8-1e1c-446f-9d08-c1e1af61925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a286c2-e8d0-4621-bfd1-d82743c7f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ram mem\n",
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc7cf3d-644a-4bfa-9fa0-b2c71796c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/vijay/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  alexnet model\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet')\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16')\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1')\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4976f5e4-f542-4903-9e7f-a83b4553e93a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1', 'features.2', 'features.3.squeeze', 'features.3.squeeze_activation', 'features.3.expand1x1', 'features.3.expand1x1_activation', 'features.3.expand3x3', 'features.3.expand3x3_activation', 'features.3.cat', 'features.4.squeeze', 'features.4.squeeze_activation', 'features.4.expand1x1', 'features.4.expand1x1_activation', 'features.4.expand3x3', 'features.4.expand3x3_activation', 'features.4.cat', 'features.5', 'features.6.squeeze', 'features.6.squeeze_activation', 'features.6.expand1x1', 'features.6.expand1x1_activation', 'features.6.expand3x3', 'features.6.expand3x3_activation', 'features.6.cat', 'features.7.squeeze', 'features.7.squeeze_activation', 'features.7.expand1x1', 'features.7.expand1x1_activation', 'features.7.expand3x3', 'features.7.expand3x3_activation', 'features.7.cat', 'features.8', 'features.9.squeeze', 'features.9.squeeze_activation', 'features.9.expand1x1', 'features.9.expand1x1_activation', 'features.9.expand3x3', 'features.9.expand3x3_activation', 'features.9.cat', 'features.10.squeeze', 'features.10.squeeze_activation', 'features.10.expand1x1', 'features.10.expand1x1_activation', 'features.10.expand3x3', 'features.10.expand3x3_activation', 'features.10.cat', 'features.11.squeeze', 'features.11.squeeze_activation', 'features.11.expand1x1', 'features.11.expand1x1_activation', 'features.11.expand3x3', 'features.11.expand3x3_activation', 'features.11.cat', 'features.12.squeeze', 'features.12.squeeze_activation', 'features.12.expand1x1', 'features.12.expand1x1_activation', 'features.12.expand3x3', 'features.12.expand3x3_activation', 'features.12.cat', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'flatten']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/vijay/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/vijay/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/vijay/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "795d18d4-c215-413d-af6e-05785634d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_layer = \"encoder.layers.encoder_layer_11.mlp\" ## ViT\n",
    "model_layer = \"features.12.cat\" ## squeezenet last layer\n",
    "\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "998bbb1f-c806-4bd6-82ff-46596ef7e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease model features\n",
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=100, batch_size=batch_size)\n",
    "\n",
    "    # Fit PCA to batch\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83597d32-85a2-451b-9e40-a565fbfd16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [08:42<00:00,  5.94s/it]\n"
     ]
    }
   ],
   "source": [
    "pca = fit_pca(feature_extractor, train_imgs_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a32ed11-4c7c-486e-8f87-bfd2719ca332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73682251-006c-4ccf-bf86-704cf0520e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [04:48<00:00,  3.28s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:31<00:00,  3.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training images features:\n",
      "(8800, 100)\n",
      "(Training stimulus images × PCA features)\n",
      "\n",
      "Validation images features:\n",
      "(984, 100)\n",
      "(Validation stimulus images × PCA features)\n",
      "\n",
      "Test images features:\n",
      "(984, 100)\n",
      "(Test stimulus images × PCA features)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features_train = extract_features(feature_extractor, train_imgs_dataloader, pca)\n",
    "features_val = extract_features(feature_extractor, val_imgs_dataloader, pca)\n",
    "features_test = extract_features(feature_extractor, test_imgs_dataloader, pca)\n",
    "\n",
    "print('\\nTraining images features:')\n",
    "print(features_train.shape)\n",
    "print('(Training stimulus images × PCA features)')\n",
    "\n",
    "print('\\nValidation images features:')\n",
    "print(features_val.shape)\n",
    "print('(Validation stimulus images × PCA features)')\n",
    "\n",
    "print('\\nTest images features:')\n",
    "print(features_val.shape)\n",
    "print('(Test stimulus images × PCA features)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4105a336-648f-4b3a-b5fb-bb4891088f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6d36d-3e1d-4796-8f80-75845c1a2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit linear regressions on the training data\n",
    "reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
    "reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
    "# Use fitted linear regressions to predict the validation and test fMRI data\n",
    "lh_fmri_val_pred = reg_lh.predict(features_val)\n",
    "lh_fmri_test_pred = reg_lh.predict(features_test)\n",
    "rh_fmri_val_pred = reg_rh.predict(features_val)\n",
    "rh_fmri_test_pred = reg_rh.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b03f54-88df-46e3-b83b-5e295f88d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(lh_fmri_val_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(rh_fmri_val_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e7865d-d4b6-48d4-8d23-ce9a9479a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = 'left' #@param ['left', 'right'] {allow-input: true}\n",
    "\n",
    "# Load the brain surface map of all vertices\n",
    "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
    "fsaverage_all_vertices = np.load(roi_dir)\n",
    "\n",
    "# Map the correlation results onto the brain surface map\n",
    "fsaverage_correlation = np.zeros(len(fsaverage_all_vertices))\n",
    "if hemisphere == 'left':\n",
    "    fsaverage_correlation[np.where(fsaverage_all_vertices)[0]] = lh_correlation\n",
    "elif hemisphere == 'right':\n",
    "    fsaverage_correlation[np.where(fsaverage_all_vertices)[0]] = rh_correlation\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_correlation,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cold_hot',\n",
    "    colorbar=True,\n",
    "    title='Encoding accuracy, '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c646e5-eea7-4af4-a056-200ffe07ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeezenet1_1\n",
    "# Load the ROI classes mapping dictionaries\n",
    "roi_mapping_files = ['mapping_prf-visualrois.npy', 'mapping_floc-bodies.npy',\n",
    "    'mapping_floc-faces.npy', 'mapping_floc-places.npy',\n",
    "    'mapping_floc-words.npy', 'mapping_streams.npy']\n",
    "roi_name_maps = []\n",
    "for r in roi_mapping_files:\n",
    "    roi_name_maps.append(np.load(os.path.join(args.data_dir, 'roi_masks', r),\n",
    "        allow_pickle=True).item())\n",
    "\n",
    "# Load the ROI brain surface maps\n",
    "lh_challenge_roi_files = ['lh.prf-visualrois_challenge_space.npy',\n",
    "    'lh.floc-bodies_challenge_space.npy', 'lh.floc-faces_challenge_space.npy',\n",
    "    'lh.floc-places_challenge_space.npy', 'lh.floc-words_challenge_space.npy',\n",
    "    'lh.streams_challenge_space.npy']\n",
    "rh_challenge_roi_files = ['rh.prf-visualrois_challenge_space.npy',\n",
    "    'rh.floc-bodies_challenge_space.npy', 'rh.floc-faces_challenge_space.npy',\n",
    "    'rh.floc-places_challenge_space.npy', 'rh.floc-words_challenge_space.npy',\n",
    "    'rh.streams_challenge_space.npy']\n",
    "lh_challenge_rois = []\n",
    "rh_challenge_rois = []\n",
    "for r in range(len(lh_challenge_roi_files)):\n",
    "    lh_challenge_rois.append(np.load(os.path.join(args.data_dir, 'roi_masks',\n",
    "        lh_challenge_roi_files[r])))\n",
    "    rh_challenge_rois.append(np.load(os.path.join(args.data_dir, 'roi_masks',\n",
    "        rh_challenge_roi_files[r])))\n",
    "\n",
    "# Select the correlation results vertices of each ROI\n",
    "roi_names = []\n",
    "lh_roi_correlation = []\n",
    "rh_roi_correlation = []\n",
    "for r1 in range(len(lh_challenge_rois)):\n",
    "    for r2 in roi_name_maps[r1].items():\n",
    "        if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n",
    "            roi_names.append(r2[1])\n",
    "            lh_roi_idx = np.where(lh_challenge_rois[r1] == r2[0])[0]\n",
    "            rh_roi_idx = np.where(rh_challenge_rois[r1] == r2[0])[0]\n",
    "            lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n",
    "            rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n",
    "roi_names.append('All vertices')\n",
    "lh_roi_correlation.append(lh_correlation)\n",
    "rh_roi_correlation.append(rh_correlation)\n",
    "\n",
    "# Create the plot\n",
    "lh_mean_roi_correlation = [np.mean(lh_roi_correlation[r])\n",
    "    for r in range(len(lh_roi_correlation))]\n",
    "rh_mean_roi_correlation = [np.mean(rh_roi_correlation[r])\n",
    "    for r in range(len(rh_roi_correlation))]\n",
    "plt.figure(figsize=(18,6))\n",
    "x = np.arange(len(roi_names))\n",
    "width = 0.30\n",
    "plt.bar(x - width/2, lh_mean_roi_correlation, width, label='Left Hemisphere')\n",
    "plt.bar(x + width/2, rh_mean_roi_correlation, width,\n",
    "    label='Right Hemishpere')\n",
    "plt.xlim(left=min(x)-.5, right=max(x)+.5)\n",
    "plt.ylim(bottom=0, top=1)\n",
    "plt.xlabel('ROIs')\n",
    "plt.xticks(ticks=x, labels=roi_names, rotation=60)\n",
    "plt.ylabel('Mean Pearson\\'s $r$')\n",
    "plt.legend(frameon=True, loc=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205091f-96b0-40f8-a1dc-a02d89ab79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test predictions\n",
    "# lh_fmri_test_pred = lh_fmri_test_pred.astype(np.float32)\n",
    "# rh_fmri_test_pred = rh_fmri_test_pred.astype(np.float32)\n",
    "# np.save(os.path.join(args.subject_submission_dir, 'lh_pred_test.npy'), lh_fmri_test_pred)\n",
    "# np.save(os.path.join(args.subject_submission_dir, 'rh_pred_test.npy'), rh_fmri_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bef504-aa01-4e80-8273-f03fa8c387e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d816631-fc7d-488f-9392-fa763c541f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
